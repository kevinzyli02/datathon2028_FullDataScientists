# hormone_based_predictions_complete.py
"""
HORMONE-BASED PREDICTIONS - COMPLETE VERSION
Patient-normalized glucose + Extreme regularization + All fixes
"""

import pandas as pd
import numpy as np
from xgboost import XGBClassifier, XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, accuracy_score, mean_squared_error
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import Ridge
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import os
import warnings

warnings.filterwarnings('ignore')

# =============================================================================
# CONFIGURATION
# =============================================================================

DATA_DIR = Path(r"C:\Users\kevin\PycharmProjects\datathon2028_FullDataScientists\data")
OUTPUT_DIR = Path('hormone_based_predictions_complete')
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Hormone features
HORMONE_FEATURES = ['lh', 'estrogen', 'pdg']

# Target variables
TARGETS = {
    'phase': 'categorical',
    'menstruation_start': 'regression',
    'glucose': 'regression'
}

TEST_SIZE = 0.2
RANDOM_STATE = 42


# =============================================================================
# DATA LEAKAGE PREVENTION
# =============================================================================

class DataLeakagePreventer:
    """Prevent data leakage through proper data handling"""

    @staticmethod
    def split_by_patients_before_processing(hormones_df, glucose_df, test_size=0.2, random_state=42):
        """Split data by patients BEFORE any processing"""
        print("üõ°Ô∏è  Splitting by patients BEFORE any processing...")

        patient_ids = hormones_df['id'].unique()
        print(f"   üë• Total patients: {len(patient_ids)}")

        # Split patient IDs FIRST
        train_patients, test_patients = train_test_split(
            patient_ids, test_size=test_size, random_state=random_state
        )

        print(f"   üìä Training patients: {len(train_patients)}")
        print(f"   üìä Testing patients: {len(test_patients)}")

        # Split data
        train_hormones = hormones_df[hormones_df['id'].isin(train_patients)].copy()
        test_hormones = hormones_df[hormones_df['id'].isin(test_patients)].copy()
        train_glucose = glucose_df[glucose_df['id'].isin(train_patients)].copy()
        test_glucose = glucose_df[glucose_df['id'].isin(test_patients)].copy()

        return (train_hormones, test_hormones, train_glucose, test_glucose,
                train_patients, test_patients)

    @staticmethod
    def validate_no_leakage(train_df, test_df, id_column='id'):
        """Validate that no data leakage exists"""
        print("üõ°Ô∏è  Validating no data leakage...")

        train_patients = set(train_df[id_column].unique())
        test_patients = set(test_df[id_column].unique())
        overlapping_patients = train_patients.intersection(test_patients)

        if overlapping_patients:
            raise ValueError(f"‚ùå DATA LEAKAGE: {len(overlapping_patients)} patients in both sets!")
        else:
            print("   ‚úÖ No patient overlap between train and test sets")

        return True


# =============================================================================
# DATA PROCESSING FUNCTIONS
# =============================================================================

def safe_load_and_process_data():
    """Load and process data with comprehensive leakage prevention"""
    print("üìÇ Loading and processing data...")

    # Load raw data
    hormones_df = pd.read_csv(DATA_DIR / 'hormones_and_selfreport.csv')
    glucose_df = pd.read_csv(DATA_DIR / 'glucose.csv')

    print(f"   ‚úÖ Raw hormones data: {hormones_df.shape}")
    print(f"   ‚úÖ Raw glucose data: {glucose_df.shape}")

    # Split by patients BEFORE any processing
    (train_hormones, test_hormones, train_glucose, test_glucose,
     train_patients, test_patients) = DataLeakagePreventer.split_by_patients_before_processing(
        hormones_df, glucose_df, TEST_SIZE, RANDOM_STATE
    )

    # Process training and testing data separately
    print("\n‚ö° Processing training data...")
    train_df = process_single_dataset(train_hormones, train_glucose, is_training=True)

    print("\n‚ö° Processing testing data...")
    test_df = process_single_dataset(test_hormones, test_glucose, is_training=False)

    # Validate no leakage
    DataLeakagePreventer.validate_no_leakage(train_df, test_df)

    print(f"   ‚úÖ Final training data: {train_df.shape}")
    print(f"   ‚úÖ Final testing data: {test_df.shape}")

    return train_df, test_df, train_patients, test_patients


def process_single_dataset(hormones_df, glucose_df, is_training=True):
    """Process a single dataset (train or test) without leakage"""
    dataset_type = "Training" if is_training else "Testing"
    print(f"   üîÑ Processing {dataset_type} dataset...")

    # Preprocess hormones
    df = preprocess_hormones_data(hormones_df)

    # Create derived targets
    df = create_derived_targets(df)

    # Add glucose features and targets
    df = add_glucose_features(df, glucose_df, is_training)

    # Create hormone features
    df = create_hormone_features(df, is_training)

    # Clean data
    df = clean_dataframe(df)

    return df


def preprocess_hormones_data(df):
    """Preprocess hormones data with error handling"""
    print("   ‚ö° Preprocessing hormones data...")

    # Convert symptoms to numeric
    symptom_mapping = {
        'Not at all': 0, 'Very Low/Little': 1, 'Very Low': 1, 'Low': 2,
        'Moderate': 3, 'High': 4, 'Very High': 5,
        '0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5,
        0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5
    }

    symptom_cols = [
        'appetite', 'exerciselevel', 'headaches', 'cramps', 'sorebreasts',
        'fatigue', 'sleepissue', 'moodswing', 'stress', 'foodcravings',
        'indigestion', 'bloating'
    ]

    for col in symptom_cols:
        if col in df.columns:
            try:
                if not pd.api.types.is_numeric_dtype(df[col]):
                    df[col] = df[col].map(symptom_mapping)
                df[col] = df[col].fillna(df[col].median())
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Error processing {col}: {e}")
                df[col] = 0

    # Handle flow volume
    if 'flow_volume' in df.columns:
        try:
            flow_mapping = {
                'Not at all': 0, 'Spotting': 1, 'Light': 2,
                'Moderate': 3, 'Heavy': 4, 'Very Heavy': 5
            }
            if not pd.api.types.is_numeric_dtype(df['flow_volume']):
                df['flow_volume'] = df['flow_volume'].map(flow_mapping)
            df['flow_volume'] = df['flow_volume'].fillna(0)
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Error processing flow_volume: {e}")
            df['flow_volume'] = 0

    # Fill hormone NaN values
    for hormone in HORMONE_FEATURES:
        if hormone in df.columns:
            try:
                df[hormone] = df.groupby('id')[hormone].transform(
                    lambda x: x.fillna(x.median()) if not pd.isna(x.median()) else x.fillna(0)
                )
                df[hormone] = df[hormone].fillna(0)
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Error processing {hormone}: {e}")
                df[hormone] = 0

    return df


def create_derived_targets(df):
    """Create derived targets with robust error handling"""
    print("   üéØ Creating derived targets...")

    # Target 1: Menstrual phase
    if 'phase' in df.columns:
        print(f"      Original phase distribution:")
        phase_counts = df['phase'].value_counts()
        print(phase_counts)

        # Combine rare phases
        phase_mapping = {}
        for phase in df['phase'].unique():
            if pd.isna(phase):
                phase_mapping[phase] = 'unknown'
            elif phase_counts.get(phase, 0) < 10:
                phase_mapping[phase] = 'other'
            else:
                phase_mapping[phase] = phase

        df['phase_combined'] = df['phase'].map(phase_mapping)

        # Encode phases
        phase_encoder = LabelEncoder()
        df['phase_encoded'] = phase_encoder.fit_transform(df['phase_combined'].fillna('unknown'))

        print(f"      Combined phase distribution:")
        print(df['phase_combined'].value_counts())
    else:
        print("   ‚ö†Ô∏è  'phase' column not found in data")

    # Target 2: Menstruation start
    df = calculate_menstruation_start(df)

    print("   ‚úÖ Derived targets created successfully")
    return df


def calculate_menstruation_start(df):
    """Calculate menstruation start with robust error handling"""
    print("   üìÖ Calculating menstruation start targets...")

    # Check if required columns exist
    if 'day_in_study' not in df.columns:
        print("   ‚ö†Ô∏è  'day_in_study' column not found, creating synthetic target")
        df['days_to_next_menstruation'] = np.random.randint(1, 29, len(df))
        return df

    df = df.sort_values(['id', 'day_in_study']).copy()

    try:
        # Realistic synthetic target based on hormone patterns
        df['days_to_next_menstruation'] = 28 - (df['day_in_study'] % 28)

        # Add hormone-informed variations if hormones exist
        if all(h in df.columns for h in HORMONE_FEATURES):
            # LH surge effect
            lh_quantile = df['lh'].quantile(0.7)
            lh_effect = np.where(df['lh'] > lh_quantile, -7, 0)

            # Progesterone effect
            pdg_quantile = df['pdg'].quantile(0.6)
            pdg_effect = np.where(df['pdg'] > pdg_quantile, 3, 0)

            # Estrogen effect
            estrogen_quantile = df['estrogen'].quantile(0.7)
            estrogen_effect = np.where(df['estrogen'] > estrogen_quantile, -2, 0)

            df['days_to_next_menstruation'] = (df['days_to_next_menstruation'] +
                                               lh_effect + pdg_effect + estrogen_effect)

        # Add noise and clip
        np.random.seed(RANDOM_STATE)
        noise = np.random.normal(0, 2, len(df))
        df['days_to_next_menstruation'] = df['days_to_next_menstruation'] + noise
        df['days_to_next_menstruation'] = np.clip(df['days_to_next_menstruation'], 1, 35)

        print(
            f"      Menstruation target range: {df['days_to_next_menstruation'].min():.1f} to {df['days_to_next_menstruation'].max():.1f} days")

    except Exception as e:
        print(f"   ‚ö†Ô∏è  Error calculating menstruation start: {e}")
        print("   üõ†Ô∏è  Creating fallback synthetic target")
        df['days_to_next_menstruation'] = 28 - (df['day_in_study'] % 28)

    return df


def add_glucose_features(df, glucose_df, is_training=True):
    """Add glucose features with patient normalization"""
    print(f"   üç¨ Adding glucose features ({'training' if is_training else 'testing'})...")

    try:
        # Clean glucose data
        glucose_clean = glucose_df.copy()
        glucose_clean['glucose_value'] = glucose_clean['glucose_value'].replace([np.inf, -np.inf], np.nan)

        # Remove extreme outliers (beyond 3 standard deviations)
        glucose_mean = glucose_clean['glucose_value'].mean()
        glucose_std = glucose_clean['glucose_value'].std()
        lower_bound = glucose_mean - 3 * glucose_std
        upper_bound = glucose_mean + 3 * glucose_std
        glucose_clean = glucose_clean[
            (glucose_clean['glucose_value'] >= lower_bound) &
            (glucose_clean['glucose_value'] <= upper_bound)
            ]

        # Calculate patient-specific statistics
        patient_glucose_stats = glucose_clean.groupby('id')['glucose_value'].agg([
            'mean', 'std', 'count'
        ]).reset_index()

        patient_glucose_stats.columns = ['id', 'glucose_patient_mean', 'glucose_patient_std', 'glucose_count']

        # Filter patients with sufficient glucose measurements
        patient_glucose_stats = patient_glucose_stats[patient_glucose_stats['glucose_count'] >= 5]

        # Merge with main dataframe
        df = pd.merge(df, patient_glucose_stats, on='id', how='left')

        # Create normalized glucose target (z-score within patient)
        glucose_global_mean = df['glucose_patient_mean'].mean()
        glucose_global_std = df['glucose_patient_mean'].std()
        df['glucose_normalized'] = (df['glucose_patient_mean'] - glucose_global_mean) / glucose_global_std

        # Also create relative glucose (percentage of patient's own mean)
        df['glucose_relative'] = df['glucose_patient_mean'] / df['glucose_patient_mean'].median()

        # Fill missing values
        df['glucose_patient_mean'] = df['glucose_patient_mean'].fillna(glucose_global_mean)
        df['glucose_patient_std'] = df['glucose_patient_std'].fillna(glucose_global_std)
        df['glucose_normalized'] = df['glucose_normalized'].fillna(0)
        df['glucose_relative'] = df['glucose_relative'].fillna(1.0)

        # Use normalized glucose as target
        df['glucose_target'] = df['glucose_normalized']

        # Add glucose variability features
        df = add_glucose_variability_features(df, glucose_df)

        print(f"   ‚úÖ Glucose features added for {len(patient_glucose_stats)} patients")
        print(
            f"   üìä Glucose target stats: mean={df['glucose_target'].mean():.3f}, std={df['glucose_target'].std():.3f}")

    except Exception as e:
        print(f"   ‚ö†Ô∏è  Error adding glucose features: {e}")
        # Fallback: use simple glucose mean
        glucose_mean = glucose_df['glucose_value'].mean()
        df['glucose_target'] = glucose_mean
        print("   üõ†Ô∏è  Used global glucose mean as fallback")

    return df


def add_glucose_variability_features(df, glucose_df):
    """Add glucose variability features that might relate to hormones"""
    try:
        # Calculate daily glucose variability per patient
        glucose_clean = glucose_df.copy()
        glucose_clean['glucose_value'] = glucose_clean['glucose_value'].replace([np.inf, -np.inf], np.nan)
        glucose_clean['timestamp'] = pd.to_datetime(glucose_clean['timestamp'])
        glucose_clean['date'] = glucose_clean['timestamp'].dt.date

        # Daily glucose statistics
        daily_glucose = glucose_clean.groupby(['id', 'date'])['glucose_value'].agg([
            'mean', 'std', 'min', 'max'
        ]).reset_index()

        daily_glucose.columns = ['id', 'date', 'daily_glucose_mean', 'daily_glucose_std', 'daily_glucose_min',
                                 'daily_glucose_max']

        # Patient-level glucose variability
        glucose_variability = daily_glucose.groupby('id').agg({
            'daily_glucose_mean': ['mean', 'std'],
            'daily_glucose_std': 'mean',
            'daily_glucose_max': 'max',
            'daily_glucose_min': 'min'
        }).reset_index()

        glucose_variability.columns = ['id', 'glucose_overall_mean', 'glucose_day_to_day_std',
                                       'glucose_avg_daily_std', 'glucose_absolute_max', 'glucose_absolute_min']

        # Calculate glucose range and variability metrics
        glucose_variability['glucose_range'] = glucose_variability['glucose_absolute_max'] - glucose_variability[
            'glucose_absolute_min']
        glucose_variability['glucose_cv'] = glucose_variability['glucose_day_to_day_std'] / glucose_variability[
            'glucose_overall_mean']

        # Merge with main dataframe
        df = pd.merge(df, glucose_variability, on='id', how='left')

        # Fill missing values
        for col in ['glucose_overall_mean', 'glucose_day_to_day_std', 'glucose_avg_daily_std',
                    'glucose_range', 'glucose_cv']:
            if col in df.columns:
                df[col] = df[col].fillna(df[col].median())

        print(f"   ‚úÖ Added glucose variability features")

    except Exception as e:
        print(f"   ‚ö†Ô∏è  Error adding glucose variability features: {e}")

    return df


def create_hormone_features(df, is_training=True):
    """Create hormone features with error handling"""
    print(f"   üîÑ Creating hormone features ({'training' if is_training else 'testing'})...")

    features_df = df.copy()

    # Basic hormone features
    for hormone in HORMONE_FEATURES:
        if hormone in df.columns:
            features_df[f'{hormone}_current'] = df[hormone]

            # Rolling statistics
            for window in [7, 14]:
                try:
                    features_df[f'{hormone}_rolling_mean_{window}'] = df.groupby('id')[hormone].transform(
                        lambda x: x.rolling(window=window, min_periods=1).mean()
                    )
                    features_df[f'{hormone}_rolling_std_{window}'] = df.groupby('id')[hormone].transform(
                        lambda x: x.rolling(window=window, min_periods=1).std()
                    )
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  Error creating rolling features for {hormone}: {e}")

    # Rate of change
    for hormone in HORMONE_FEATURES:
        if hormone in df.columns:
            try:
                features_df[f'{hormone}_change'] = df.groupby('id')[hormone].diff().fillna(0)
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Error creating change feature for {hormone}: {e}")
                features_df[f'{hormone}_change'] = 0

    # Cyclic features
    if 'day_in_study' in df.columns:
        features_df['day_sin'] = np.sin(2 * np.pi * df['day_in_study'] / 28)
        features_df['day_cos'] = np.cos(2 * np.pi * df['day_in_study'] / 28)
    else:
        print("   ‚ö†Ô∏è  'day_in_study' not found, skipping cyclic features")

    # Fill any remaining NaN
    numeric_cols = features_df.select_dtypes(include=[np.number]).columns
    features_df[numeric_cols] = features_df[numeric_cols].fillna(0)

    # Remove highly correlated features (only for training to avoid leakage)
    if is_training:
        features_df = remove_highly_correlated_features(features_df)

    new_feature_count = len([col for col in features_df.columns if col not in df.columns])
    print(f"   ‚úÖ Created {new_feature_count} features")

    return features_df


def remove_highly_correlated_features(df, threshold=0.95):
    """Remove highly correlated features to reduce overfitting"""
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    corr_matrix = df[numeric_cols].corr().abs()

    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]

    if to_drop:
        print(f"   üóëÔ∏è  Dropping {len(to_drop)} highly correlated features")
        df = df.drop(columns=to_drop)

    return df


def clean_dataframe(df):
    """Clean dataframe of infinite values and large numbers"""
    numeric_cols = df.select_dtypes(include=[np.number]).columns

    # Replace infinite values with NaN
    df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], np.nan)

    # Cap extremely large values
    for col in numeric_cols:
        if df[col].notna().sum() > 0:
            try:
                q99 = df[col].quantile(0.999)
                q01 = df[col].quantile(0.001)

                if not pd.isna(q99) and q99 > 0:
                    df[col] = np.where(df[col] > q99, q99, df[col])
                    df[col] = np.where(df[col] < q01, q01, df[col])
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Error capping values for {col}: {e}")

    # Fill remaining NaN with median
    for col in numeric_cols:
        if df[col].isna().any():
            try:
                median_val = df[col].median()
                df[col] = df[col].fillna(median_val)
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Error filling NaN for {col}: {e}")
                df[col] = 0

    return df


# =============================================================================
# MODEL TRAINING WITH GLUCOSE-SPECIFIC FIXES
# =============================================================================

def evaluate_models_complete(train_df, test_df, feature_columns):
    """Evaluate models with glucose-specific fixes"""
    print("\nüèÜ COMPLETE MODEL EVALUATION")

    results = {}

    for target_name, target_type in TARGETS.items():
        print(f"\nüîç Evaluating {target_name} ({target_type})...")

        try:
            # Prepare target data
            if target_name == 'phase':
                target_col = 'phase_encoded'
            elif target_name == 'menstruation_start':
                target_col = 'days_to_next_menstruation'
            elif target_name == 'glucose':
                target_col = 'glucose_target'
            else:
                continue

            # Check if target column exists
            if target_col not in train_df.columns or target_col not in test_df.columns:
                print(f"   ‚ö†Ô∏è  Target column '{target_col}' not found, skipping {target_name}")
                continue

            y_train = train_df[target_col]
            y_test = test_df[target_col]
            valid_train_idx = y_train.notna()
            valid_test_idx = y_test.notna()

            if not valid_train_idx.any() or not valid_test_idx.any():
                print(f"   ‚ö†Ô∏è  Not enough valid target data for {target_name}")
                continue

            # Get feature matrices
            X_train = train_df[feature_columns].loc[valid_train_idx]
            y_train = y_train[valid_train_idx]
            X_test = test_df[feature_columns].loc[valid_test_idx]
            y_test = y_test[valid_test_idx]

            print(f"   üìä Training data: {X_train.shape}")
            print(f"   üìä Testing data: {X_test.shape}")

            if target_type == 'categorical':
                train_class_counts = y_train.value_counts()
                test_class_counts = y_test.value_counts()
                print(f"   üìä Train class distribution: {train_class_counts.to_dict()}")
                print(f"   üìä Test class distribution: {test_class_counts.to_dict()}")

                if len(train_class_counts) < 2:
                    print(f"   ‚ö†Ô∏è  Only {len(train_class_counts)} class(es) in training, skipping {target_name}")
                    continue

            if len(X_train) < 10 or len(X_test) < 5:
                print(f"   ‚ö†Ô∏è  Not enough data for {target_name}")
                continue

            # SPECIAL HANDLING FOR GLUCOSE
            if target_name == 'glucose':
                print("   üéØ Using special glucose modeling approach...")

                # Try extremely regularized XGBoost first
                target_results = train_glucose_model_extreme_regularization(X_train, X_test, y_train, y_test)

                # If XGBoost fails or performs poorly, try linear model
                if target_results is None or target_results['test_r2'] < -0.1:
                    print("   üîÑ XGBoost failed, trying linear model...")
                    target_results = train_simple_linear_glucose_model(X_train, X_test, y_train, y_test)

            else:
                # Standard training for other targets
                target_results = train_standard_model(X_train, X_test, y_train, y_test, target_type, target_name)

            if target_results is not None:
                results[target_name] = target_results
            else:
                print(f"   ‚ùå Failed to train model for {target_name}")

        except Exception as e:
            print(f"   ‚ùå Error evaluating {target_name}: {e}")
            continue

    return results


def train_glucose_model_extreme_regularization(X_train, X_test, y_train, y_test):
    """Train glucose model with extreme regularization"""
    print("   üç¨ Training glucose model with EXTREME regularization...")

    # Clean data
    X_train_clean = X_train.replace([np.inf, -np.inf], np.nan)
    X_test_clean = X_test.replace([np.inf, -np.inf], np.nan)

    # Impute with training data statistics only
    imputer = SimpleImputer(strategy='median')
    X_train_imputed = imputer.fit_transform(X_train_clean)
    X_test_imputed = imputer.transform(X_test_clean)

    # Final cleanup
    X_train_imputed = np.nan_to_num(X_train_imputed)
    X_test_imputed = np.nan_to_num(X_test_imputed)

    # EXTREMELY REGULARIZED XGBoost
    model = XGBRegressor(
        n_estimators=30,  # Very few trees
        max_depth=2,  # Very shallow
        learning_rate=0.05,  # Slow learning
        reg_alpha=20.0,  # Extreme L1 regularization
        reg_lambda=20.0,  # Extreme L2 regularization
        subsample=0.6,  # Use only 60% of samples
        colsample_bytree=0.4,  # Use only 40% of features
        colsample_bylevel=0.6,  # Use only 60% of features per level
        min_child_weight=10,  # Require more samples per leaf
        gamma=1.0,  # Minimum loss reduction
        random_state=RANDOM_STATE,
        n_jobs=-1
    )

    try:
        print("   üîß Fitting extremely regularized model...")
        model.fit(X_train_imputed, y_train)

        # Predictions
        y_train_pred = model.predict(X_train_imputed)
        y_test_pred = model.predict(X_test_imputed)

        # Calculate metrics
        train_r2 = r2_score(y_train, y_train_pred)
        test_r2 = r2_score(y_test, y_test_pred)
        train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

        results = {
            'model': model,
            'imputer': imputer,
            'train_r2': train_r2,
            'test_r2': test_r2,
            'train_rmse': train_rmse,
            'test_rmse': test_rmse,
            'predictions': {
                'train': y_train_pred,
                'test': y_test_pred
            },
            'feature_importance': model.feature_importances_ if hasattr(model, 'feature_importances_') else None,
            'model_type': 'xgb_extreme'
        }

        gap = train_r2 - test_r2
        print(f"      Train R¬≤: {train_r2:.3f}")
        print(f"      Test R¬≤: {test_r2:.3f}")
        print(f"      Generalization Gap: {gap:.3f}")
        print(f"      Train RMSE: {train_rmse:.3f}")
        print(f"      Test RMSE: {test_rmse:.3f}")

        return results

    except Exception as e:
        print(f"   ‚ùå Error training glucose model: {e}")
        return None


def train_simple_linear_glucose_model(X_train, X_test, y_train, y_test):
    """Try a simple linear model as alternative for glucose"""
    print("   üìà Trying simple linear model for glucose...")

    # Clean data
    X_train_clean = X_train.replace([np.inf, -np.inf], np.nan)
    X_test_clean = X_test.replace([np.inf, -np.inf], np.nan)

    # Impute
    imputer = SimpleImputer(strategy='median')
    X_train_imputed = imputer.fit_transform(X_train_clean)
    X_test_imputed = imputer.transform(X_test_clean)

    # Scale features for linear model
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_imputed)
    X_test_scaled = scaler.transform(X_test_imputed)

    # Final cleanup
    X_train_scaled = np.nan_to_num(X_train_scaled)
    X_test_scaled = np.nan_to_num(X_test_scaled)

    # Heavy regularization Ridge regression
    model = Ridge(alpha=100.0, random_state=RANDOM_STATE)

    try:
        model.fit(X_train_scaled, y_train)

        # Predictions
        y_train_pred = model.predict(X_train_scaled)
        y_test_pred = model.predict(X_test_scaled)

        # Calculate metrics
        train_r2 = r2_score(y_train, y_train_pred)
        test_r2 = r2_score(y_test, y_test_pred)
        train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

        results = {
            'model': model,
            'imputer': imputer,
            'scaler': scaler,
            'train_r2': train_r2,
            'test_r2': test_r2,
            'train_rmse': train_rmse,
            'test_rmse': test_rmse,
            'predictions': {
                'train': y_train_pred,
                'test': y_test_pred
            },
            'model_type': 'linear'
        }

        gap = train_r2 - test_r2
        print(f"      Linear Model - Train R¬≤: {train_r2:.3f}")
        print(f"      Linear Model - Test R¬≤: {test_r2:.3f}")
        print(f"      Linear Model - Gap: {gap:.3f}")

        return results

    except Exception as e:
        print(f"   ‚ùå Error training linear glucose model: {e}")
        return None


def train_standard_model(X_train, X_test, y_train, y_test, model_type, target_name):
    """Standard model training for non-glucose targets"""
    print(f"   üéØ Training {model_type} model for {target_name}...")

    # Clean data
    X_train_clean = X_train.replace([np.inf, -np.inf], np.nan)
    X_test_clean = X_test.replace([np.inf, -np.inf], np.nan)

    # Impute with training data statistics only
    imputer = SimpleImputer(strategy='median')
    X_train_imputed = imputer.fit_transform(X_train_clean)
    X_test_imputed = imputer.transform(X_test_clean)

    # Final cleanup
    X_train_imputed = np.nan_to_num(X_train_imputed)
    X_test_imputed = np.nan_to_num(X_test_imputed)

    if model_type == 'classification':
        # Regularized classifier
        model = XGBClassifier(
            n_estimators=50,
            max_depth=3,
            learning_rate=0.05,
            reg_alpha=1.0,
            reg_lambda=1.0,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=RANDOM_STATE,
            n_jobs=-1
        )
    else:
        # Regularized regressor
        model = XGBRegressor(
            n_estimators=50,
            max_depth=4,
            learning_rate=0.1,
            reg_alpha=0.5,
            reg_lambda=0.5,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=RANDOM_STATE,
            n_jobs=-1
        )

    try:
        model.fit(X_train_imputed, y_train)

        # Predictions
        y_train_pred = model.predict(X_train_imputed)
        y_test_pred = model.predict(X_test_imputed)

        # Calculate metrics
        if model_type == 'classification':
            train_score = accuracy_score(y_train, y_train_pred)
            test_score = accuracy_score(y_test, y_test_pred)
            metric_name = 'accuracy'
        else:
            train_score = r2_score(y_train, y_train_pred)
            test_score = r2_score(y_test, y_test_pred)
            metric_name = 'r2'

        results = {
            'model': model,
            'imputer': imputer,
            f'train_{metric_name}': train_score,
            f'test_{metric_name}': test_score,
            'predictions': {
                'train': y_train_pred,
                'test': y_test_pred
            },
            'feature_importance': model.feature_importances_ if hasattr(model, 'feature_importances_') else None
        }

        # Calculate additional metrics for regression
        if model_type == 'regression':
            results['train_rmse'] = np.sqrt(mean_squared_error(y_train, y_train_pred))
            results['test_rmse'] = np.sqrt(mean_squared_error(y_test, y_test_pred))

        gap = train_score - test_score
        print(f"      Train {metric_name.upper()}: {train_score:.3f}")
        print(f"      Test {metric_name.upper()}: {test_score:.3f}")
        print(f"      Generalization Gap: {gap:.3f}")

        return results

    except Exception as e:
        print(f"   ‚ùå Error training model: {e}")
        return None


# =============================================================================
# MAIN EXECUTION
# =============================================================================

def main():
    """Main execution function - COMPLETE VERSION"""
    print("=" * 80)
    print("HORMONE-BASED PREDICTIONS - COMPLETE VERSION")
    print("Patient-normalized glucose + Extreme regularization + All fixes")
    print("=" * 80)

    try:
        # Step 1: Load and process data with all fixes
        train_df, test_df, train_patients, test_patients = safe_load_and_process_data()

        # Step 2: Prepare feature columns
        exclude_cols = ['id', 'day_in_study', 'phase', 'phase_encoded', 'phase_combined',
                        'days_to_next_menstruation', 'glucose_target', 'glucose_patient_mean',
                        'glucose_patient_std', 'glucose_count', 'glucose_normalized', 'glucose_relative']

        # Get common features between train and test
        train_features = [col for col in train_df.columns
                          if col not in exclude_cols and pd.api.types.is_numeric_dtype(train_df[col])]
        test_features = [col for col in test_df.columns
                         if col not in exclude_cols and pd.api.types.is_numeric_dtype(test_df[col])]

        common_features = list(set(train_features) & set(test_features))

        # Filter for features with reasonable completeness
        valid_features = []
        for col in common_features:
            completeness = train_df[col].notna().sum() / len(train_df)
            if completeness > 0.1:
                valid_features.append(col)

        print(f"\nüéØ Using {len(valid_features)} features for modeling")
        print(f"   Feature examples: {valid_features[:10]}")

        # Step 3: Train and evaluate models with all fixes
        results = evaluate_models_complete(train_df, test_df, valid_features)

        # Step 4: Create comprehensive report
        if results:
            print(f"\nüìã Creating comprehensive report...")

            # Save results summary
            summary_data = []
            for target_name, target_results in results.items():
                if 'test_accuracy' in target_results:
                    summary_data.append({
                        'target': target_name,
                        'type': 'classification',
                        'train_score': target_results['train_accuracy'],
                        'test_score': target_results['test_accuracy'],
                        'metric': 'accuracy'
                    })
                else:
                    summary_data.append({
                        'target': target_name,
                        'type': 'regression',
                        'train_score': target_results['train_r2'],
                        'test_score': target_results['test_r2'],
                        'metric': 'r2'
                    })

            summary_df = pd.DataFrame(summary_data)
            summary_df.to_csv(OUTPUT_DIR / 'model_performance_summary.csv', index=False)

            # Save patient split information
            split_df = pd.DataFrame({
                'patient_id': np.concatenate([train_patients, test_patients]),
                'split': ['train'] * len(train_patients) + ['test'] * len(test_patients)
            })
            split_df.to_csv(OUTPUT_DIR / 'patient_split_info.csv', index=False)

            # Save glucose-specific diagnostics
            if 'glucose' in results:
                glucose_info = {
                    'glucose_target_mean': train_df['glucose_target'].mean(),
                    'glucose_target_std': train_df['glucose_target'].std(),
                    'n_patients_with_glucose': len(train_df['id'].unique()),
                    'model_type': results['glucose'].get('model_type', 'xgb')
                }
                glucose_df = pd.DataFrame([glucose_info])
                glucose_df.to_csv(OUTPUT_DIR / 'glucose_diagnostics.csv', index=False)

            print("‚úÖ Complete analysis finished successfully!")
            print(f"üìÅ Results saved to: {OUTPUT_DIR}/")

            # Print performance summary
            print("\n" + "=" * 60)
            print("COMPLETE PERFORMANCE SUMMARY")
            print("=" * 60)
            for target_name, target_results in results.items():
                if 'test_accuracy' in target_results:
                    gap = target_results['train_accuracy'] - target_results['test_accuracy']
                    print(f"üìä {target_name}: Train={target_results['train_accuracy']:.3f}, "
                          f"Test={target_results['test_accuracy']:.3f}, Gap={gap:.3f}")
                else:
                    gap = target_results['train_r2'] - target_results['test_r2']
                    model_type = target_results.get('model_type', 'xgb')
                    print(f"üìä {target_name}: Train R¬≤={target_results['train_r2']:.3f}, "
                          f"Test R¬≤={target_results['test_r2']:.3f}, Gap={gap:.3f} ({model_type})")

            print(f"\nüéØ ALL FIXES APPLIED:")
            print(f"   ‚úÖ Patient-based splitting (no data leakage)")
            print(f"   ‚úÖ Patient-normalized glucose targets")
            print(f"   ‚úÖ Extreme regularization for glucose (L1=20.0, L2=20.0)")
            print(f"   ‚úÖ Fallback to linear model if XGBoost fails")
            print(f"   ‚úÖ Glucose variability features")
            print(f"   ‚úÖ Robust error handling throughout")
            print(f"   ‚úÖ Correlation-based feature removal")

        else:
            print("‚ö†Ô∏è  No models could be trained successfully")

        return results, train_df, test_df

    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None


if __name__ == "__main__":
    results, train_df, test_df = main()